# RT-2:视觉语言行动模型将网页知识转移到机器人控制

我们研究了如何将在互联网规模数据上训练的视觉语言模型直接融入端到端的机器人控制,以提高泛化能力并实现新兴的语义推理。我们的目标是使单个端到端训练的模型既能学习将机器人观察映射到动作,又能从网页上的语言和视觉语言数据中获得大规模预训练的好处。为此,我们提出了同时在机器人轨迹数据和互联网规模的视觉语言任务(如视觉问答)上联合微调最先进的视觉语言模型的方法。与其他方法不同,我们提出了一个简单通用的配方来实现这一目标:为了将自然语言响应和机器人动作统一到相同的格式,我们将动作表示为文本标记,并以与自然语言标记相同的方式直接将其并入模型的训练数据集。我们将这类模型称为视觉语言行动模型(VLA),并实例化了这样一个模型,称为RT-2。我们广泛的评估(6000次评估试验)表明,我们的方法导致了高性能的机器人策略,并使RT-2能够从互联网规模训练中获得一系列新兴能力。这包括明显改进的对新对象的泛化能力、解释机器人训练数据中不存在的命令(如将对象放在特定数字或图标上)的能力,以及响应用户命令进行基本推理(如捡起最小或最大的对象,或距离另一个对象最近的对象)的能力。我们进一步表明,加入思维链推理使RT-2能够执行多阶段语义推理,例如弄清楚拿起哪个对象作为improvised锤子(一块石头),或者哪种饮料最适合疲惫的人(能量饮料)。


## 1. 引言

在广泛的网络规模数据集上预训练的高容量模型为广泛的下游任务提供了一个有效且强大的平台:

大规模语言模型不仅可以实现流畅的文本生成(Anil等,2023; Brohan等,2022; OpenAI,2023),

还可以实现新兴的问题解决(Cobbe等,2021; Lewkowycz等,2022; Polu等,2022)和

创造性的散文(Brown等,2020; OpenAI,2023)和

代码(Chen等,2021)生成,

而视觉语言模型可以实现开放词汇的视觉识别(Kirillov等,2023; Minderer等,2022; Radford等,2021),

甚至可以对图像中的对象-代理交互进行复杂的推理(Alayrac等,2022; Chen等,2023a,b; Driess等,2023; Hao等,2022; Huang等,2023; Wang等,2022)。

这种语义推理、问题解决和视觉解释能力对必须在真实环境中执行各种任务的通用机器人将非常有用。

然而,机器人应该如何获得这些能力还不清楚。虽然一种蛮力方法可能涉及收集数百万次机器人交互试验,但最 capable 语言和视觉语言模型是在网络上训练的数十亿个标记和图像(Alayrac等,2022; Chen等,2023a,b; Huang等,2023),这不太可能在近期内用机器人数据来匹配。另一方面,直接将这些模型应用于机器人任务也很困难:这些模型推理语义、标签和文本提示,而机器人需要接地的低级动作,比如笛卡尔式末端执行器命令。尽管近些年有许多工作试图将语言模型(LLM)和视觉语言模型(VLM)整合到机器人中(Ahn等,2022; Driess等,2023; Vemprala等,2023),但这些方法通常只解决机器人规划的“更高层次”方面,实际上扮演着状态机的角色,解释命令并将其解析为各个基元(如抓取和放置对象),然后由单独的低级控制器执行这些基元,而这些控制器在训练期间本身并没有从互联网规模模型的丰富语义知识中获得好处。因此,在本文中,我们问:大规模预训练的视觉语言模型能否直接集成到低级机器人控制中,以提升泛化能力并实现新兴的语义推理?


为此,我们探索了一种简单且出乎意料地有效的方法:我们直接训练设计用于开放词汇的视觉问答和视觉对话的视觉语言模型输出低级机器人动作,同时解决其他互联网规模的视觉语言任务。虽然这种模型通常被训练来产生自然语言标记,但我们可以通过将动作标记化为文本标记并创建“多模态句子”(Driess等,2023)在机器人轨迹上训练它们,这些句子通过产生相应的动作来“回应”配对的机器人指令和相机观察。通过这种方式,视觉语言模型可以直接训练为遵循指令的机器人策略。

这种简单的方法与先前将VLM融入机器人策略(Shridhar等,2022a)或
从零开始设计新的视觉语言动作体系结构(Reed等,2022)的替代方法形成对比

:预存在的视觉语言模型经过训练可以在不增加任何新参数的情况下输出文本编码的动作,这些模型已经进行了大量的运算投资。我们将这类模型称为视觉语言动作(VLA)模型。我们通过构建RT-1(Brohan等,2022)提出的协议来实例化VLA模型,使用类似的数据集,但将模型扩展为使用大型视觉语言 backbone。因此,我们将我们的模型称为RT-2(机器人变压器2)。我们在图1中给出概述。